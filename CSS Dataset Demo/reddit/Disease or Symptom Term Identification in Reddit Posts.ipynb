{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text  Label\n",
      "0  Corona and mental health. Indiana's 211 hotlin...      1\n",
      "1  Impact of genetic mutations on cocaine addicti...      2\n",
      "2  Oakland on Tuesday became the second U.S. city...      1\n",
      "3  MDMA treatment for alcoholism reduces relapse ...      1\n",
      "4  'I was non-stop Juuling up a storm': 10 colleg...      2\n",
      "1    3430\n",
      "2    3360\n",
      "0    3225\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('RHMD_3_Class.csv')\n",
    "\n",
    "# Explore the dataset\n",
    "print(data.head())\n",
    "print(data['Label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\ABINAYA\n",
      "[nltk_data]     THULSI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\ABINAYA\n",
      "[nltk_data]     THULSI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Text preprocessing\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    # Stemming or lemmatization can be added here if needed\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "data['Cleaned Text'] = data['Text'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TF-IDF vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = tfidf_vectorizer.fit_transform(data['Cleaned Text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6704942586120819\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.71      0.70       648\n",
      "           1       0.72      0.60      0.66       702\n",
      "           2       0.61      0.71      0.65       653\n",
      "\n",
      "    accuracy                           0.67      2003\n",
      "   macro avg       0.68      0.67      0.67      2003\n",
      "weighted avg       0.68      0.67      0.67      2003\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, data['Label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Naive Bayes classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\ABINAYA\n",
      "[nltk_data]     THULSI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\ABINAYA\n",
      "[nltk_data]     THULSI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6809785322016975\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.73      0.72       648\n",
      "           1       0.69      0.60      0.64       702\n",
      "           2       0.64      0.71      0.68       653\n",
      "\n",
      "    accuracy                           0.68      2003\n",
      "   macro avg       0.68      0.68      0.68      2003\n",
      "weighted avg       0.68      0.68      0.68      2003\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('RHMD_3_Class.csv')\n",
    "\n",
    "# Text preprocessing\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "data['Cleaned Text'] = data['Text'].apply(preprocess_text)\n",
    "\n",
    "# TF-IDF vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = tfidf_vectorizer.fit_transform(data['Cleaned Text'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, data['Label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Random Forest Classifier (you can try other classifiers as well)\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\ABINAYA\n",
      "[nltk_data]     THULSI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\ABINAYA\n",
      "[nltk_data]     THULSI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6674987518721918\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.76      0.71       648\n",
      "           1       0.70      0.58      0.64       702\n",
      "           2       0.65      0.67      0.66       653\n",
      "\n",
      "    accuracy                           0.67      2003\n",
      "   macro avg       0.67      0.67      0.67      2003\n",
      "weighted avg       0.67      0.67      0.67      2003\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('RHMD_3_Class.csv')\n",
    "\n",
    "# Text preprocessing\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "data['Cleaned Text'] = data['Text'].apply(preprocess_text)\n",
    "\n",
    "# TF-IDF vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))\n",
    "X = tfidf_vectorizer.fit_transform(data['Cleaned Text'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, data['Label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Random Forest Classifier\n",
    "classifier = RandomForestClassifier(n_estimators=200, max_depth=30, random_state=42, class_weight='balanced')\n",
    "\n",
    "# Train the classifier\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Disease or Symptom Term Identification in Reddit Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Data Preprocessing\n",
    "\n",
    "#Load the dataset from the CSV file, which contains columns for Reddit posts and labels (0, 1, or 2).\n",
    "#Preprocess the text data by removing punctuation, converting text to lowercase, and tokenizing the text into words.\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('RHMD_3_Class.csv')\n",
    "\n",
    "# Data preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "data['Reddit posts'] = data['Text'].apply(preprocess_text)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['Text'], data['Label'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Feature Extraction\n",
    "\n",
    "#Use techniques like TF-IDF (Term Frequency-Inverse Document Frequency) to convert the text data into numerical features.\n",
    "#Extract features from both the Reddit posts and the disease/symptom terms you want to identify\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TF-IDF vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Define the list of disease/symptom terms to identify\n",
    "disease_symptom_terms = [\"Addiction\",\"Allergic\",\"Alzheimer\",\"Asthma\",\"Cancer\",\"Cough\",\"Depression\",\"Diabetes\",\"Fever\",\"Headache\",\"Heart attack\",\"Migraine\",\"OCD\",\"PTSD\",\"Stroke\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3: Train a Classification Model\n",
    "\n",
    "#Train a text classification model, such as a logistic regression or a random forest classifier, to predict whether each Reddit post contains disease or symptom terms (binary classification: 0 for no, 1 for yes).\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train a logistic regression classifier\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = classifier.predict(X_test_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7054418372441338\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.75      0.74       648\n",
      "           1       0.72      0.67      0.69       702\n",
      "           2       0.67      0.70      0.69       653\n",
      "\n",
      "    accuracy                           0.71      2003\n",
      "   macro avg       0.71      0.71      0.71      2003\n",
      "weighted avg       0.71      0.71      0.71      2003\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Step 4: Evaluate the Model\n",
    "\n",
    "#Evaluate the model's performance using metrics such as accuracy, precision, recall, and F1-score.\n",
    "#Analyze the model's ability to identify disease or symptom terms in Reddit posts.\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "report = classification_report(y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
